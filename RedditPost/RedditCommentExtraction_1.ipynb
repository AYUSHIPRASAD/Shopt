{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0N4GbjDmysV",
        "outputId": "930e9b0d-00d8-49d0-a235-ff32fbbd7900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw unidecode pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8OH-4kX8QOV",
        "outputId": "963c3212-5b99-4b9d-8f27-13d9f3183d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " All comments saved.\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"praw\").setLevel(logging.ERROR)\n",
        "\n",
        "# Load Product Names from Excel\n",
        "file_path = \"/content/Headphones.xlsx\"\n",
        "sheet_name = \"Headphone-Avail-Price\"\n",
        "\n",
        "# Read product names from the specified sheet\n",
        "df_products = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "# Ensure \"Product Name\" column exists\n",
        "if \"Product Name\" not in df_products.columns:\n",
        "    raise ValueError(\" 'Product Name' column not found in the Excel sheet.\")\n",
        "\n",
        "# Extract product names\n",
        "products = df_products[\"Product Name\"].dropna().unique().tolist()\n",
        "\n",
        "#  Reddit API Credentials (Ensure these are correct and not shared publicly)\n",
        "reddit = praw.Reddit(\n",
        "    client_id='jNAMMjqIULpTFdVMVeVxkg',\n",
        "    client_secret='S5lIYbBLN0WDcoKvoYUgKzF3p2gB7g',\n",
        "    user_agent='MyShopt by /u/TemporaryTop5884'\n",
        ")\n",
        "\n",
        "def get_all_comments_from_top_posts(search_query, min_comments=100, comments_per_post=None, wait_time=3):\n",
        "\n",
        "    all_comments = []\n",
        "    total_comments = 0\n",
        "\n",
        "    while total_comments < min_comments:\n",
        "        #  Fetch top Reddit posts for the query\n",
        "        submissions = list(reddit.subreddit(\"all\").search(\n",
        "            search_query,\n",
        "            sort=\"relevance\",\n",
        "            syntax=\"lucene\",\n",
        "            time_filter=\"all\",\n",
        "            limit=100  # Fetch up to 100 posts per query\n",
        "        ))\n",
        "\n",
        "        if not submissions:\n",
        "            break\n",
        "\n",
        "        #  Process each submission\n",
        "        for submission in submissions:\n",
        "\n",
        "            # Load full submission with comments\n",
        "            full_submission = reddit.submission(id=submission.id)\n",
        "            full_submission.comments.replace_more(limit=5)  # Expand comments\n",
        "            flat_comments = full_submission.comments.list()\n",
        "\n",
        "            #  Apply per-post comment limit (if set)\n",
        "            if comments_per_post:\n",
        "                flat_comments = flat_comments[:comments_per_post]\n",
        "\n",
        "            #  Store comments\n",
        "            for comment in flat_comments:\n",
        "                all_comments.append({\n",
        "                    \"product\": search_query,\n",
        "                    \"post_title\": submission.title,\n",
        "                    \"post_id\": submission.id,\n",
        "                    \"post_url\": f\"https://www.reddit.com{submission.permalink}\",\n",
        "                    \"post_score\": submission.score,\n",
        "                    \"post_created_utc\": datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    \"subreddit\": str(submission.subreddit),\n",
        "\n",
        "                    \"comment_id\": comment.id,\n",
        "                    \"comment_author\": str(comment.author),\n",
        "                    \"comment_body\": comment.body,\n",
        "                    \"comment_score\": comment.score,\n",
        "                    \"comment_created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    \"comment_url\": f\"https://www.reddit.com{comment.permalink}\",\n",
        "                    \"comment_depth\": comment.depth,\n",
        "                    \"is_top_level\": comment.depth == 0\n",
        "                })\n",
        "                total_comments += 1\n",
        "\n",
        "                # Stop if we hit the minimum required comments\n",
        "                if total_comments >= min_comments:\n",
        "                    break\n",
        "\n",
        "            if total_comments >= min_comments:\n",
        "                break\n",
        "\n",
        "        # Avoid rate limits\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "    return pd.DataFrame(all_comments)\n",
        "\n",
        "# Fetch & Save Data for Each Product\n",
        "all_products_comments = []\n",
        "\n",
        "#  Fetch Data for Each Product\n",
        "for product in products:\n",
        "    search_query = f\"Review {product} headphone\"\n",
        "    comments_df = get_all_comments_from_top_posts(search_query)\n",
        "\n",
        "    if not comments_df.empty:\n",
        "        all_products_comments.append(comments_df)\n",
        "    else:\n",
        "        print(f\" No comments found for {product}.\")\n",
        "\n",
        "#  Combine all product comments into a single DataFrame\n",
        "if all_products_comments:\n",
        "    combined_df = pd.concat(all_products_comments, ignore_index=True)\n",
        "\n",
        "    # Save as a single Excel file\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"All_Headphone_Reviews_Reddit_Comments_{timestamp}.xlsx\"\n",
        "    combined_df.to_excel(filename, index=False)\n",
        "\n",
        "    # Optionally save each product as a separate sheet in the same Excel file\n",
        "    with pd.ExcelWriter(f\"Headphone_Reviews_By_Product_{timestamp}.xlsx\") as writer:\n",
        "        for product in products:\n",
        "            product_df = combined_df[combined_df['product'] == f\"Review {product} headphone\"]\n",
        "            if not product_df.empty:\n",
        "                # Create sheet name (Excel has 31 character limit for sheet names)\n",
        "                sheet_name = product[:31]\n",
        "                product_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "else:\n",
        "    print(\"\\n No comments found for any products.\")\n",
        "\n",
        "print(\"\\n All comments saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZG9xOEAHc5D",
        "outputId": "5631b20b-2a2b-4b67-f8e4-0a7e6946626d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall numpy gensim -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2TO9it6LgJN",
        "outputId": "e95fcf35-7a75-430c-dea3-d447f2839971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0DmGL5avGO2",
        "outputId": "00298d29-a393-4802-b8ca-7b2a8d9b7da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoDl_4WLJLQA",
        "outputId": "1a1307f8-456f-45a2-8e16-433611573047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: gensim\n",
            "Version: 4.3.3\n",
            "Summary: Python framework for fast Vector Space Modelling\n",
            "Home-page: https://radimrehurek.com/gensim/\n",
            "Author: Radim Rehurek\n",
            "Author-email: me@radimrehurek.com\n",
            "License: LGPL-2.1-only\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: numpy, scipy, smart-open\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJjC30hUKzSq",
        "outputId": "1bbd76b1-8a8c-4705-8e6c-d60d1f8e5e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSA7Ql-XkC1V",
        "outputId": "d8a6a565-21a9-4b76-cc05-ba68980abe19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Sennheiser HD 800 S: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Processing Sony WH-1000XM4: 100%|██████████| 1/1 [00:03<00:00,  3.16s/it]\n",
            "Processing Beyerdynamic Amiron Wireless: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
            "Processing Focal Stellia: 100%|██████████| 1/1 [00:03<00:00,  3.72s/it]\n",
            "Processing Audeze LCD-X: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
            "Processing Bose QuietComfort Ultra: 100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n",
            "Processing Grado SR325x: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
            "Processing HiFiMan Arya: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
            "Processing Apple AirPods Max: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
            "Processing Shure SE846: 100%|██████████| 1/1 [00:00<00:00,  5.36it/s]\n",
            "Processing Dan Clark Audio Ether 2: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
            "Processing Bowers & Wilkins PX7 S2: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "Processing Stax SR-009S: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Processing Meze Empyrean: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Processing ZMF Verite Closed: 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n",
            "Processing Philips Fidelio X3: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
            "Processing JBL Tour One M2: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
            "Processing Sony WH-1000XM5: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
            "Processing Apple AirPods Pro 2: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n",
            "Processing Bose QuietComfort 45: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
            "Processing Sennheiser Momentum 4 Wireless: 100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n",
            "Processing Beats Studio Pro: 100%|██████████| 1/1 [00:03<00:00,  3.14s/it]\n",
            "Processing Bose Noise Cancelling Headphones 700: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
            "Processing Shure AONIC 50 Gen 2: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
            "Processing Microsoft Surface Headphones 2: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
            "Processing Jabra Elite 85h: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
            "Processing Anker Soundcore Space Q45: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
            "Processing Sennheiser PXC 550-II: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
            "Processing Shure AONIC 50: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
            "Processing Plantronics BackBeat Pro 2: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
            "Processing Bowers & Wilkins PX8: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
            "Processing Beats Studio3 Wireless: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
            "Processing V-MODA Crossfade M-100 Master: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
            "Processing Anker Soundcore Life Q30: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Processing Skullcandy Crusher Evo: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
            "Processing Samsung Galaxy Buds2 Pro: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
            "Processing Bowers & Wilkins Px7 S2e: 100%|██████████| 1/1 [00:00<00:00,  7.29it/s]\n",
            "Processing Skullcandy Hesh ANC: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
            "Processing Technics EAH-A800: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
            "Processing 1More SonoFlow: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n",
            "Processing Edifier WH950NB: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
            "Processing Sony WF-1000XM4: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
            "Processing Sennheiser HD 450BT: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
            "Processing Samsung Galaxy Buds Pro: 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]\n",
            "Processing Bose QuietComfort Earbuds II: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
            "Processing Technics EAH-AZ80: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]\n",
            "Processing Grado GT220: 100%|██████████| 1/1 [00:00<00:00, 10.50it/s]\n",
            "Processing JBL Tour Pro 2: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
            "Processing Audio-Technica ATH-TWX9: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
            "Processing Audio-Technica ATH-M50x: 100%|██████████| 1/1 [00:03<00:00,  3.83s/it]\n",
            "Processing Shure SRH1540: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
            "Processing Beyerdynamic Amiron Home: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Processing Focal Clear MG: 100%|██████████| 1/1 [00:02<00:00,  2.86s/it]\n",
            "Processing AKG K702: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
            "Processing Beyerdynamic DT 990 Pro: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
            "Processing AirPods Max: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Processing HiFiMan Sundara: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
            "Processing Philips Fidelio X2HR: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Processing Meze 99 Classics: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\n",
            "Processing ZMF Auteur: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it]\n",
            "Processing Focal Clear: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n",
            "Processing Sennheiser HD 650: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "Processing Beyerdynamic DT 880 Pro: 100%|██████████| 1/1 [00:00<00:00, 10.57it/s]\n",
            "Processing AKG K701: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
            "Processing HiFiMAN HE-400i: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "Processing Dan Clark Audio Ether Flow: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
            "Processing Jabra Elite 7 Pro: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
            "Processing HiFiMan Ananda BT: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n",
            "Processing Focal Bathys: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "Processing Master & Dynamic MW75: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\n",
            "Processing Klipsch T5 II: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
            "Processing Sennheiser Momentum 3 Wireless: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
            "Processing AKG N700NC M2: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
            "Processing Turtle Beach Stealth 700 Gen 2: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
            "Processing Sony WH-CH710N: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
            "Processing HyperX Cloud II Wireless: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]\n",
            "Processing Jabra Evolve 75: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "Processing SteelSeries Arctis Pro Wireless: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
            "Processing Razer BlackShark V2 Pro: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Processing Logitech Zone Wireless: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
            "Processing Astro A50 Wireless: 100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
            "Processing Jabra Elite Active 75t: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]\n",
            "Processing Sony WF-SP800N: 100%|██████████| 1/1 [00:00<00:00,  7.03it/s]\n",
            "Processing Beats Powerbeats Pro: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
            "Processing AfterShokz Aeropex: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
            "Processing Bose Sport Earbuds: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
            "Processing Shokz OpenMove: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
            "Processing 1MORE ComfoBuds Mini: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Processing Creative Outlier Pro: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
            "Processing TOZO T10: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\n",
            "Processing Marshall Major IV: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\n",
            "Processing SteelSeries Arctis Nova Pro Wireless: 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]\n",
            "Processing Logitech G Pro X Wireless: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
            "Processing EPOS H3Pro Hybrid: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n",
            "Processing Audio-Technica ATH-G1WL: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
            "Processing Beyerdynamic MMX 300 (2nd Gen): 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
            "Processing Lucidsound LS50X: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
            "Processing Grado GW100: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
            "Processing JBL Tune 760NC: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
            "Processing Creative Zen Hybrid: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n",
            "Processing Jabra Elite 10: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
            "Processing Audio-Technica ATH-M50xBT2: 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n",
            "Processing Dali IO-12: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
            "Processing Grado GW100x: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure required NLTK data packages are available\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Define a set of stop words for filtering common words in text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "cached_embeddings = {}  # Cache embeddings to avoid redundant computations\n",
        "\n",
        "# File and sheet details for product specifications\n",
        "headphone_file = \"/content/Headphones.xlsx\"\n",
        "sheet_name = \"Headphone-Avail-Price\"\n",
        "\n",
        "def safe_string(text):\n",
        "   ## Converts input text to a string safely, returning an empty string if the input is NaN.\n",
        "\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    return str(text)\n",
        "\n",
        "def contains_product_name(text, title, product_name):\n",
        "    ## Checks if the product name appears in either the text or the title. It also checks for partial matches of the product name components.\n",
        "\n",
        "    text = safe_string(text).lower()\n",
        "    title = safe_string(title).lower()\n",
        "    product_name = safe_string(product_name).lower()\n",
        "\n",
        "    if product_name in text or product_name in title:\n",
        "        return True\n",
        "\n",
        "    product_parts = product_name.split()\n",
        "    # Ensure at least one match is required (can adjust threshold if needed)\n",
        "    required_matches = max(1, len(product_parts) // 1)\n",
        "\n",
        "    text_matches = sum(1 for part in product_parts\n",
        "                      if len(part) > 2 and part not in stop_words and part in text)\n",
        "\n",
        "    title_matches = sum(1 for part in product_parts\n",
        "                       if len(part) > 2 and part not in stop_words and part in title)\n",
        "\n",
        "    return (text_matches >= required_matches) or (title_matches >= required_matches)\n",
        "\n",
        "def extract_key_terms(description):\n",
        "    ## Extracts key terms from the product description by tokenizing, converting to lower case, and removing punctuation and stop words.\n",
        "\n",
        "    text = safe_string(description).lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = word_tokenize(text)\n",
        "    key_terms = [word for word in words if word.isalnum()\n",
        "                and word not in stop_words\n",
        "                and len(word) > 2]\n",
        "    return set(key_terms)\n",
        "\n",
        "def extract_feature_phrases(text):\n",
        "    ## Extracts potential feature phrases from the product description.\n",
        "    # It splits the text by newlines and applies various heuristics to capture important features and phrases.\n",
        "\n",
        "    text = safe_string(text)\n",
        "    lines = text.split('\\n')\n",
        "    features = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        # Remove list markers or numbers at the start of the line\n",
        "        line = re.sub(r'^[•\\-\\*\\d]+\\.?\\s*', '', line)\n",
        "        if ':' in line:\n",
        "            parts = line.split(':', 1)\n",
        "            feature = parts[0].strip()\n",
        "            if feature and len(feature.split()) <= 5:\n",
        "                features.append(feature.lower())\n",
        "            features.append(line.lower())\n",
        "        else:\n",
        "            if len(line.split()) <= 10:\n",
        "                features.append(line.lower())\n",
        "\n",
        "    # If few features are detected, try extracting key terms and bigrams from combined text\n",
        "    if len(features) < 3 and len(lines) >= 3:\n",
        "        combined_text = ' '.join(lines)\n",
        "        tokens = word_tokenize(combined_text.lower())\n",
        "        important_terms = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "        important_bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)\n",
        "                           if tokens[i].isalnum() and tokens[i+1].isalnum()\n",
        "                           and tokens[i] not in stop_words]\n",
        "        features.extend(important_terms[:10])\n",
        "        features.extend(important_bigrams[:5])\n",
        "\n",
        "    return list(set(features))\n",
        "\n",
        "def compute_feature_overlap(comment, product_features):\n",
        "    ## Calculates a simple overlap score between the comment and product features.\n",
        "\n",
        "    comment_text = safe_string(comment).lower()\n",
        "    matches = 0\n",
        "    for feature in product_features:\n",
        "        if feature in comment_text:\n",
        "            matches += 1\n",
        "            continue\n",
        "        feature_parts = feature.split()\n",
        "        if any(part in comment_text for part in feature_parts if len(part) > 3 and part not in stop_words):\n",
        "            matches += 0.5\n",
        "            continue\n",
        "    return matches\n",
        "\n",
        "def get_embedding(text, model):\n",
        "\n",
        "    ## Retrieves the embedding for the given text using the provided model.\n",
        "\n",
        "    text = safe_string(text)\n",
        "    if text in cached_embeddings:\n",
        "        return cached_embeddings[text]\n",
        "    embedding = model.encode(text)\n",
        "    cached_embeddings[text] = embedding\n",
        "    return embedding\n",
        "\n",
        "def is_generic_or_bot_comment(text):\n",
        "    ## Checks if a comment is generic or appears to be generated by a bot.\n",
        "\n",
        "    text = safe_string(text).lower()\n",
        "\n",
        "    # Check if the comment mentions a bot\n",
        "    if re.search(r'\\bbot\\b', text) is not None:\n",
        "        return True\n",
        "\n",
        "    # Patterns that indicate transaction tracking or generic bot messages\n",
        "    transaction_patterns = [\n",
        "        r'this comment is now being tracked',\n",
        "        r'your flair will update',\n",
        "        r'please reply to the above comment with your feedback',\n",
        "        r'once you reply, you will both get credit',\n",
        "        r'if you did \\*\\*not\\*\\* complete a transaction',\n",
        "        r'thank you!\\s*---\\s*\\[\\^'\n",
        "    ]\n",
        "\n",
        "    for pattern in transaction_patterns:\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def is_quality_comment(comment, min_words=15, min_sentences=2):\n",
        "    ## Determines if a comment meets quality criteria based on its word count and sentence structure.\n",
        "    #Removes URLs and formatting symbols before evaluation.\n",
        "\n",
        "    comment = safe_string(comment)\n",
        "    if not comment:\n",
        "        return False\n",
        "    # Remove URLs from the comment\n",
        "    comment_text = re.sub(r'https?://\\S+|www\\.\\S+', '', comment)\n",
        "    # Remove common markdown symbols and punctuation that may interfere with sentence tokenization\n",
        "    comment_text = re.sub(r'[*_~`#\\[\\]\\(\\)\\{\\}>]', '', comment_text)\n",
        "    words = [w for w in word_tokenize(comment_text.lower())\n",
        "             if w.isalnum() and w not in stop_words]\n",
        "    sentences = sent_tokenize(comment_text)\n",
        "    return (len(words) >= min_words and\n",
        "            len(sentences) >= min_sentences and\n",
        "            any(len(s.split()) > 5 for s in sentences))\n",
        "\n",
        "def product_focused_similarity(product_description, comment, product_name, product_features, model):\n",
        "    ## Computes a similarity score between a product's description and a comment.\n",
        "\n",
        "    product_description = safe_string(product_description)\n",
        "    comment = safe_string(comment)\n",
        "    product_name = safe_string(product_name)\n",
        "    # Calculate how many product features overlap with the comment text\n",
        "    feature_overlap = compute_feature_overlap(comment, product_features)\n",
        "    # Get embeddings for semantic similarity measurement\n",
        "    prod_embedding = get_embedding(product_description, model)\n",
        "    comment_embedding = get_embedding(comment, model)\n",
        "    semantic_sim = cosine_similarity([prod_embedding], [comment_embedding])[0][0]\n",
        "    # Check if the product name is mentioned in the comment (full or partial bonus)\n",
        "    product_mention = 1 if product_name.lower() in comment.lower() else 0.5\n",
        "    final_score = (0.4 * semantic_sim +\n",
        "                  0.4 * min(1.0, feature_overlap / max(1, len(product_features) / 3)) +\n",
        "                  0.2 * product_mention)\n",
        "    return {\n",
        "        'final_score': final_score,\n",
        "        'semantic_similarity': semantic_sim,\n",
        "        'feature_overlap': feature_overlap,\n",
        "        'product_mention': product_mention\n",
        "    }\n",
        "\n",
        "def score_and_rank_comments(df, product_description, product_name, model, batch_size=100):\n",
        "    ## Filters and ranks comments related to a product.\n",
        "\n",
        "    # Exclude comments from AutoModerator and generic/bot comments\n",
        "    if 'comment_author' in df.columns:\n",
        "        df = df[\n",
        "            (df['comment_author'] != 'AutoModerator') &\n",
        "            (~df['comment_body'].apply(lambda x: is_generic_or_bot_comment(x)))\n",
        "        ].copy()\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Helper to safely retrieve values from a DataFrame row\n",
        "    def safe_get(row, column, default=\"\"):\n",
        "        if column in row and not pd.isna(row[column]):\n",
        "            return row[column]\n",
        "        return default\n",
        "\n",
        "    # Label comments that are of sufficient quality\n",
        "    df['is_quality'] = df.apply(\n",
        "        lambda row: (\n",
        "            is_quality_comment(safe_get(row, 'comment_body')) and\n",
        "            not is_generic_or_bot_comment(safe_get(row, 'comment_body'))\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "    filtered_df = df[df['is_quality']].copy()\n",
        "    if filtered_df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter comments that mention the product name in text or title\n",
        "    filtered_df['mentions_product'] = filtered_df.apply(\n",
        "        lambda row: contains_product_name(\n",
        "            safe_get(row, 'comment_body'),\n",
        "            safe_get(row, 'post_title'),\n",
        "            product_name\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    product_mention_df = filtered_df[filtered_df['mentions_product']].copy()\n",
        "    if product_mention_df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Extract key terms and features from the product description\n",
        "    key_terms = extract_key_terms(product_description)\n",
        "    product_features = extract_feature_phrases(product_description)\n",
        "\n",
        "    total_rows = len(product_mention_df)\n",
        "    scores = []\n",
        "\n",
        "    # Process comments in batches to show progress using tqdm\n",
        "    for i in tqdm(range(0, total_rows, batch_size), desc=f\"Processing {product_name}\"):\n",
        "        batch = product_mention_df.iloc[i:min(i+batch_size, total_rows)]\n",
        "        batch_scores = [\n",
        "            product_focused_similarity(\n",
        "                product_description,\n",
        "                safe_get(row, 'comment_body'),\n",
        "                product_name,\n",
        "                product_features,\n",
        "                model\n",
        "            ) for _, row in batch.iterrows()\n",
        "        ]\n",
        "        scores.extend(batch_scores)\n",
        "\n",
        "    score_df = pd.DataFrame(scores)\n",
        "    result_df = pd.concat([product_mention_df.reset_index(drop=True), score_df], axis=1)\n",
        "    result_df = result_df.sort_values(by='final_score', ascending=False)\n",
        "    # result_df = result_df[result_df['final_score'] > 0.3]\n",
        "\n",
        "    # Mark whether the product is mentioned in the post title\n",
        "    result_df['product_in_title'] = result_df.apply(\n",
        "        lambda row: product_name.lower() in safe_string(row.get('post_title', '')).lower() or\n",
        "                   any(part in safe_string(row.get('post_title', '')).lower()\n",
        "                      for part in product_name.lower().split()\n",
        "                      if len(part) > 2 and part not in stop_words),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Mark whether the product is mentioned in the comment body\n",
        "    result_df['product_in_body'] = result_df.apply(\n",
        "        lambda row: product_name.lower() in safe_string(row.get('comment_body', '')).lower() or\n",
        "                   any(part in safe_string(row.get('comment_body', '')).lower()\n",
        "                      for part in product_name.lower().split()\n",
        "                      if len(part) > 2 and part not in stop_words),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main routine:\n",
        "    1. Loads the sentence transformer model.\n",
        "    2. Finds the latest comments file.\n",
        "    3. Reads product specifications.\n",
        "    4. Scores and ranks comments per product.\n",
        "    5. Outputs the top 5 unique comments per product into an Excel file.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Locate the most recent comments file\n",
        "    possible_files = glob.glob(\"All_Headphone_Reviews_Reddit_Comments_*.xlsx\")\n",
        "    if not possible_files:\n",
        "        return\n",
        "\n",
        "    latest_file = max(possible_files, key=os.path.getmtime)\n",
        "\n",
        "    try:\n",
        "        comments_df = pd.read_excel(latest_file)\n",
        "    except Exception as e:\n",
        "        return\n",
        "\n",
        "    # Check for required columns; rename if necessary\n",
        "    required_columns = ['comment_body', 'post_title', 'comment_author']\n",
        "    missing_columns = [col for col in required_columns if col not in comments_df.columns]\n",
        "    if missing_columns:\n",
        "        column_mapping = {}\n",
        "        for req_col in missing_columns:\n",
        "            potential_matches = [col for col in comments_df.columns\n",
        "                                if req_col.lower() in col.lower() or\n",
        "                                col.lower() in req_col.lower()]\n",
        "            if potential_matches:\n",
        "                column_mapping[req_col] = potential_matches[0]\n",
        "        if column_mapping:\n",
        "            comments_df = comments_df.rename(columns={v: k for k, v in column_mapping.items()})\n",
        "\n",
        "    try:\n",
        "        # Read product specifications from the Excel file\n",
        "        spec_df = pd.read_excel(\"Headphones.xlsx\", sheet_name=\"Headphone-Avail-Price\")\n",
        "        if not {'Product Name', 'Specifications'}.issubset(spec_df.columns):\n",
        "            return\n",
        "        products = dict(zip(spec_df['Product Name'].astype(str), spec_df['Specifications'].astype(str)))\n",
        "    except Exception as e:\n",
        "        return\n",
        "\n",
        "    ranked_comments = {}\n",
        "    # Process comments for each product\n",
        "    for pn in products.keys():\n",
        "        ranked_df = score_and_rank_comments(comments_df, products[pn], pn, model)\n",
        "        if len(ranked_df) > 0:\n",
        "            ranked_comments[pn] = ranked_df\n",
        "        else:\n",
        "            ranked_comments[pn] = pd.DataFrame()\n",
        "\n",
        "    summary_data = []\n",
        "    # For each product, select the top 5 unique comments based on the final score\n",
        "    for pn, df in ranked_comments.items():\n",
        "        if not df.empty:\n",
        "            unique_comments = df.drop_duplicates(subset='comment_body')\n",
        "            top_comments = unique_comments.sort_values(by='final_score', ascending=False).head(5)\n",
        "            top_comments['product_name'] = pn\n",
        "            summary_data.append(top_comments)\n",
        "\n",
        "    if summary_data:\n",
        "        all_ranked_top_comments = pd.concat(summary_data)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"top5_unique_comments_per_product_{timestamp}.xlsx\"\n",
        "        all_ranked_top_comments.to_excel(filename, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
