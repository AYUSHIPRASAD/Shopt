# -*- coding: utf-8 -*-
"""RedditCommentExtraction_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rPWPC3SBs_7dC0T4a-SSurNjEWv5TWpX
"""

!pip install praw unidecode pandas openpyxl

import praw
import pandas as pd
from datetime import datetime
import time

# üîπ Load Product Names from Excel
file_path = "/content/Headphones.xlsx"
sheet_name = "Headphone-Avail-Price"

# Read product names from the specified sheet
df_products = pd.read_excel(file_path, sheet_name=sheet_name)

# Ensure "Product Name" column exists
if "Product Name" not in df_products.columns:
    raise ValueError("‚ùå 'Product Name' column not found in the Excel sheet.")

# Extract product names
products = df_products["Product Name"].dropna().unique().tolist()

# üîπ Reddit API Credentials (Ensure these are correct and not shared publicly)
reddit = praw.Reddit(
    client_id='jNAMMjqIULpTFdVMVeVxkg',
    client_secret='S5lIYbBLN0WDcoKvoYUgKzF3p2gB7g',
    user_agent='MyShopt by /u/TemporaryTop5884'
)

def get_all_comments_from_top_posts(search_query, min_comments=100, comments_per_post=None, wait_time=3):

    all_comments = []
    total_comments = 0

    while total_comments < min_comments:
        # üîπ Fetch top Reddit posts for the query
        submissions = list(reddit.subreddit("all").search(
            search_query,
            sort="relevance",
            syntax="lucene",
            time_filter="all",
            limit=100  # Fetch up to 100 posts per query
        ))

        if not submissions:
            print(f"‚ùå No results found for '{search_query}'. Skipping...")
            break

        # üîπ Process each submission
        for submission in submissions:
            print(f"üìå Processing post: {submission.title}")

            # Load full submission with comments
            full_submission = reddit.submission(id=submission.id)
            full_submission.comments.replace_more(limit=5)  # Expand comments
            flat_comments = full_submission.comments.list()

            # üîπ Apply per-post comment limit (if set)
            if comments_per_post:
                flat_comments = flat_comments[:comments_per_post]

            # üîπ Store comments
            for comment in flat_comments:
                all_comments.append({
                    "product": search_query,
                    "post_title": submission.title,
                    "post_id": submission.id,
                    "post_url": f"https://www.reddit.com{submission.permalink}",
                    "post_score": submission.score,
                    "post_created_utc": datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                    "subreddit": str(submission.subreddit),

                    "comment_id": comment.id,
                    "comment_author": str(comment.author),
                    "comment_body": comment.body,
                    "comment_score": comment.score,
                    "comment_created_utc": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                    "comment_url": f"https://www.reddit.com{comment.permalink}",
                    "comment_depth": comment.depth,
                    "is_top_level": comment.depth == 0
                })
                total_comments += 1

                # Stop if we hit the minimum required comments
                if total_comments >= min_comments:
                    break

            if total_comments >= min_comments:
                break

        # Avoid rate limits
        time.sleep(wait_time)

    return pd.DataFrame(all_comments)

# üîπ Fetch & Save Data for Each Product
all_products_comments = []

# üîπ Fetch Data for Each Product
for product in products:
    search_query = f"Review {product} headphone"
    print(f"\nüöÄ Searching for: {search_query} ...")
    comments_df = get_all_comments_from_top_posts(search_query)

    if not comments_df.empty:
        all_products_comments.append(comments_df)
        print(f"‚úÖ Found {len(comments_df)} comments for {product}")
    else:
        print(f"‚ö†Ô∏è No comments found for {product}.")

# üîπ Combine all product comments into a single DataFrame
if all_products_comments:
    combined_df = pd.concat(all_products_comments, ignore_index=True)

    # Save as a single Excel file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"All_Headphone_Reviews_Reddit_Comments_{timestamp}.xlsx"
    combined_df.to_excel(filename, index=False)
    print(f"\n‚úÖ Saved all {len(combined_df)} comments to: {filename}")

    # Optionally save each product as a separate sheet in the same Excel file
    with pd.ExcelWriter(f"Headphone_Reviews_By_Product_{timestamp}.xlsx") as writer:
        for product in products:
            product_df = combined_df[combined_df['product'] == f"Review {product} headphone"]
            if not product_df.empty:
                # Create sheet name (Excel has 31 character limit for sheet names)
                sheet_name = product[:31]
                product_df.to_excel(writer, sheet_name=sheet_name, index=False)
        print(f"‚úÖ Also saved comments by product in separate sheets")
else:
    print("\n‚ö†Ô∏è No comments found for any products.")

print("\nüéâ Done! All comments saved.")

!pip uninstall numpy gensim -y

!pip install --upgrade numpy

!pip install --upgrade gensim

!pip show gensim

!python --version

#product_relevant_comments_20250401_233547
######THE BESTTTTTTT
import os
import glob
import re
import nltk
import string
import numpy as np
import pandas as pd
import textwrap
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('tokenizers/punkt_tab') # Check if punkt_tab is downloaded.
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('punkt_tab') # Download punkt_tab.

# Global variables for caching
stop_words = set(stopwords.words('english'))
cached_embeddings = {}

headphone_file = "/content/Headphones.xlsx"
sheet_name = "Headphone-Avail-Price"

def safe_string(text):
    """Convert any input to a string safely"""
    if pd.isna(text):
        return ""
    return str(text)

def contains_product_name(text, title, product_name):
    """
    Check if product name is mentioned in either the text or title
    Returns True only if at least part of the product name is found
    """
    text = safe_string(text).lower()
    title = safe_string(title).lower()
    product_name = safe_string(product_name).lower()

    # Check for exact product name
    if product_name in text or product_name in title:
        return True

    # Check for product components
    product_parts = product_name.split()

    required_matches = max(1, len(product_parts) // 1)

    # Count matches in text
    text_matches = sum(1 for part in product_parts
                      if len(part) > 2 and part not in stop_words and part in text)

    # Count matches in title
    title_matches = sum(1 for part in product_parts
                       if len(part) > 2 and part not in stop_words and part in title)

    # Check if we have enough matches in either text or title
    return (text_matches >= required_matches) or (title_matches >= required_matches)

def extract_key_terms(description):
    """Extract key product terms from the description"""
    text = safe_string(description).lower()

    # Remove punctuation and tokenize
    text = text.translate(str.maketrans('', '', string.punctuation))
    words = word_tokenize(text)

    # Remove stopwords and keep only alphanumeric words
    key_terms = [word for word in words if word.isalnum()
                and word not in stop_words
                and len(word) > 2]  # Filter out very short words

    # Return unique terms
    return set(key_terms)

def extract_feature_phrases(text):
    """Extract feature phrases from product description"""
    text = safe_string(text)
    lines = text.split('\n')

    features = []
    for line in lines:
        line = line.strip()
        if not line:
            continue

        # Remove any leading identifiers (like bullets, numbers)
        line = re.sub(r'^[‚Ä¢\-\*\d]+\.?\s*', '', line)

        # Check for key feature indicators
        if ':' in line:
            # Split by colon and take the meaningful part
            parts = line.split(':', 1)
            feature = parts[0].strip()
            if feature and len(feature.split()) <= 5:  # Reasonable feature name length
                features.append(feature.lower())

            # Also add the full line
            features.append(line.lower())
        else:
            # Add full line as a feature if not too long
            if len(line.split()) <= 10:
                features.append(line.lower())

    # Add additional features that are part of a list or technical specs
    if len(features) < 3 and len(lines) >= 3:
        # This might be a paragraph description, try to extract noun phrases
        combined_text = ' '.join(lines)
        tokens = word_tokenize(combined_text.lower())
        important_terms = [word for word in tokens if word.isalnum() and word not in stop_words]
        important_bigrams = [f"{tokens[i]} {tokens[i+1]}" for i in range(len(tokens)-1)
                           if tokens[i].isalnum() and tokens[i+1].isalnum()
                           and tokens[i] not in stop_words]

        features.extend(important_terms[:10])  # Add top single terms
        features.extend(important_bigrams[:5])  # Add some bigrams

    return list(set(features))  # Remove duplicates

def compute_feature_overlap(comment, product_features):
    """Compute how many product features are mentioned in the comment"""
    comment_text = safe_string(comment).lower()

    # Direct substring match (exact feature mentioned)
    matches = 0
    for feature in product_features:
        if feature in comment_text:
            matches += 1
            continue

        # Check for key parts of multi-word features (e.g., "noise cancellation" -> "cancellation")
        feature_parts = feature.split()
        if any(part in comment_text for part in feature_parts if len(part) > 3 and part not in stop_words):
            matches += 0.5
            continue

    return matches

def get_embedding(text, model):
    """Get embedding with caching"""
    text = safe_string(text)

    if text in cached_embeddings:
        return cached_embeddings[text]

    embedding = model.encode(text)
    cached_embeddings[text] = embedding
    return embedding

def is_quality_comment(comment, min_words=15, min_sentences=2):
     # Check if a comment meets quality criteria for length and substance
    # Convert to string and handle NaN values
    comment = safe_string(comment)

    if not comment:
        return False

    # Remove URLs
    comment_text = re.sub(r'https?://\S+|www\.\S+', '', comment)

    # Remove markdown formatting
    comment_text = re.sub(r'[*_~`#\[\]\(\)\{\}>]', '', comment_text)

    # Count words (excluding stopwords)
    words = [w for w in word_tokenize(comment_text.lower())
             if w.isalnum() and w not in stop_words]

    # Count sentences
    sentences = sent_tokenize(comment_text)

    # Check quality criteria
    return (len(words) >= min_words and
            len(sentences) >= min_sentences and
            any(len(s.split()) > 5 for s in sentences))  # At least one substantial sentence

def product_focused_similarity(product_description, comment, product_name, product_features, model):
    # Calculate product-focused similarity score
    product_description = safe_string(product_description)
    comment = safe_string(comment)
    product_name = safe_string(product_name)

    # Calculate feature overlap
    feature_overlap = compute_feature_overlap(comment, product_features)

    # Calculate semantic similarity
    prod_embedding = get_embedding(product_description, model)
    comment_embedding = get_embedding(comment, model)
    semantic_sim = cosine_similarity([prod_embedding], [comment_embedding])[0][0]

    # Product mention factor
    product_mention = 1 if product_name.lower() in comment.lower() else 0.5

    # Calculate final score with emphasis on feature overlap
    final_score = (0.4 * semantic_sim +
                  0.4 * min(1.0, feature_overlap / max(1, len(product_features) / 3)) +
                  0.2 * product_mention)

    return {
        'final_score': final_score,
        'semantic_similarity': semantic_sim,
        'feature_overlap': feature_overlap,
        'product_mention': product_mention
    }

def score_and_rank_comments(df, product_description, product_name, model, batch_size=100):
    # Score and rank comments with progress bar and batching
    # First, filter out AutoModerator comments
    if 'comment_author' in df.columns:
        initial_count = len(df)
        df = df[df['comment_author'] != 'AutoModerator'].copy()
        print(f"Removed {initial_count - len(df)} AutoModerator comments")
    else:
        print("Warning: 'comment_author' column not found, can't filter AutoModerator comments")

    print(f"Original comment count: {len(df)}")

    # Check column existence and create safe getters
    def safe_get(row, column, default=""):
        if column in row and not pd.isna(row[column]):
            return row[column]
        return default

    # Apply quality filter
    df['is_quality'] = df.apply(
        lambda row: is_quality_comment(safe_get(row, 'comment_body')),
        axis=1
    )

    filtered_df = df[df['is_quality']].copy()
    print(f"Quality comment count: {len(filtered_df)} (removed {len(df) - len(filtered_df)} low-quality comments)")

    if filtered_df.empty:
        return pd.DataFrame()  # Return empty frame if no good comments

    # Check for product name mention in title or comment
    filtered_df['mentions_product'] = filtered_df.apply(
        lambda row: contains_product_name(
            safe_get(row, 'comment_body'),
            safe_get(row, 'post_title'),
            product_name
        ),
        axis=1
    )

    # Filter to only include comments with product mentions
    product_mention_df = filtered_df[filtered_df['mentions_product']].copy()
    print(f"Comments mentioning product: {len(product_mention_df)} (removed {len(filtered_df) - len(product_mention_df)} without product mention)")

    if product_mention_df.empty:
        return pd.DataFrame()  # Return empty frame if no comments mention the product

    # Extract product features once
    key_terms = extract_key_terms(product_description)
    product_features = extract_feature_phrases(product_description)
    print(f"Extracted {len(key_terms)} key terms and {len(product_features)} features")
    print(f"Sample features: {', '.join(product_features[:3])}")

    total_rows = len(product_mention_df)
    scores = []

    # Process in batches to show progress
    for i in tqdm(range(0, total_rows, batch_size), desc=f"Processing {product_name}"):
        batch = product_mention_df.iloc[i:min(i+batch_size, total_rows)]
        batch_scores = [
            product_focused_similarity(
                product_description,
                safe_get(row, 'comment_body'),
                product_name,
                product_features,
                model
            ) for _, row in batch.iterrows()
        ]
        scores.extend(batch_scores)

    score_df = pd.DataFrame(scores)
    result_df = pd.concat([product_mention_df.reset_index(drop=True), score_df], axis=1)

    # Sort and filter by minimum score
    result_df = result_df.sort_values(by='final_score', ascending=False)
    result_df = result_df[result_df['final_score'] > 0.3]  # Only keep reasonably relevant comments

    # Mark how product was mentioned (in title, body, or both)
    result_df['product_in_title'] = result_df.apply(
        lambda row: product_name.lower() in safe_string(row.get('post_title', '')).lower() or
                   any(part in safe_string(row.get('post_title', '')).lower()
                      for part in product_name.lower().split()
                      if len(part) > 2 and part not in stop_words),
        axis=1
    )

    result_df['product_in_body'] = result_df.apply(
        lambda row: product_name.lower() in safe_string(row.get('comment_body', '')).lower() or
                   any(part in safe_string(row.get('comment_body', '')).lower()
                      for part in product_name.lower().split()
                      if len(part) > 2 and part not in stop_words),
        axis=1
    )

    return result_df

def format_comment(text, max_length=None):
    # Format a comment for display with word wrap
    if max_length and len(text) > max_length:
        return text[:max_length] + "..."
    return text

def print_top_comments(product_name, comments_df, count=5):
    # Print the top comments for a product with nice formatting
    if comments_df.empty:
        print(f"\n{'='*80}")
        print(f"No relevant comments found for {product_name}")
        print(f"{'='*80}")
        return

    print(f"\n{'='*80}")
    print(f"TOP {min(count, len(comments_df))} COMMENTS FOR: {product_name}")
    print(f"{'='*80}")

    top_comments = comments_df.head(count)

    for i, (_, comment) in enumerate(top_comments.iterrows(), 1):
        print(f"\n{i}. Score: {comment['final_score']:.2f} | Features: {comment['feature_overlap']:.1f}")

        # Show where product is mentioned
        mention_locations = []
        if comment.get('product_in_title', False):
            mention_locations.append("title")
        if comment.get('product_in_body', False):
            mention_locations.append("body")
        mention_str = f"Product mentioned in: {', '.join(mention_locations)}"
        print(f"   {mention_str}")

        print(f"   {'-'*76}")

        # Print post title
        post_title = safe_string(comment.get('post_title', ''))
        if post_title:
            print(f"   Title: {post_title[:70]}")
            print()

        # Get comment text
        comment_text = safe_string(comment.get('comment_body', ''))

        # Print wrapped comment text with proper indentation
        lines = textwrap.wrap(comment_text, width=75)
        for line in lines:
            print(f"   {line}")

        print(f"   {'-'*76}")

def main():
    # Load model once
    print("Loading sentence transformer model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Look for both stroller and headphone files
    possible_files = glob.glob("All_Headphone_Reviews_Reddit_Comments_*.xlsx")
    if not possible_files:
        print("No comment files found. Please make sure your data file exists.")
        return

    # Use the most recent file
    latest_file = max(possible_files, key=os.path.getmtime)
    print(f"Loading comments from: {latest_file}")

    # Print columns to help diagnose issues
    try:
        comments_df = pd.read_excel(latest_file)
        print(f"Loaded {len(comments_df)} comments")
        print(f"DataFrame columns: {comments_df.columns.tolist()}")
    except Exception as e:
        print(f"Error reading Excel file: {e}")
        return

    # Check for required columns
    required_columns = ['comment_body', 'post_title', 'comment_author']
    missing_columns = [col for col in required_columns if col not in comments_df.columns]
    if missing_columns:
        print(f"Warning: Missing required columns: {missing_columns}")
        print("Available columns:", comments_df.columns.tolist())
        # Try to map similar column names
        column_mapping = {}
        for req_col in missing_columns:
            potential_matches = [col for col in comments_df.columns
                                if req_col.lower() in col.lower() or
                                col.lower() in req_col.lower()]
            if potential_matches:
                column_mapping[req_col] = potential_matches[0]
                print(f"Using '{potential_matches[0]}' for '{req_col}'")

        # Rename columns if possible
        if column_mapping:
            comments_df = comments_df.rename(columns={v: k for k, v in column_mapping.items()})

    # Product definitions
    try:
        spec_df = pd.read_excel("Headphones.xlsx", sheet_name="Headphone-Avail-Price")
        print("Loaded product specifications from 'Headphones.xlsx'.")

        # Ensure necessary columns exist
        if not {'Product Name', 'Specifications'}.issubset(spec_df.columns):
            print("Missing required columns in 'Headphone-Avail-Price' sheet.")
            return

        # Create dictionary of product names to specifications
        products = dict(zip(spec_df['Product Name'].astype(str), spec_df['Specifications'].astype(str)))
        print(f"Loaded {len(products)} product definitions.")

    except Exception as e:
        print(f"Error reading product specifications: {e}")
        return

    # Process each product and print top 5 comments
    ranked_comments = {}
    for pn in products.keys():
        print(f"\nProcessing {pn}...")
        ranked_df = score_and_rank_comments(comments_df, products[pn], pn, model)

        if len(ranked_df) > 0:
            ranked_comments[pn] = ranked_df
            print(f"Found {len(ranked_df)} relevant comments for {pn}")
        else:
            ranked_comments[pn] = pd.DataFrame()  # Empty DataFrame
            print(f"No relevant comments found for {pn}")

        # Print top 5 comments right away
        print_top_comments(pn, ranked_df, count=5)

    summary_data = []
    for pn, df in ranked_comments.items():
        if not df.empty:
            # Drop duplicate comments
            unique_comments = df.drop_duplicates(subset='comment_body')

            # Sort and take top 5
            top_comments = unique_comments.sort_values(by='final_score', ascending=False).head(5)

            # Collect for final output
            top_comments['product_name'] = pn
            summary_data.append(top_comments)

    # Save to Excel
    if summary_data:
        all_ranked_top_comments = pd.concat(summary_data)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"top5_unique_comments_per_product_{timestamp}.xlsx"
        all_ranked_top_comments.to_excel(filename, index=False)
        print(f"\n‚úÖ Saved top 5 unique comments per product to: {filename}")
    else:
        print("\nNo relevant comments found for any product.")

if __name__ == "__main__":
    main()



